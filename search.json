[
  {
    "objectID": "posts/our_first_times/index.html",
    "href": "posts/our_first_times/index.html",
    "title": "Our First Times",
    "section": "",
    "text": "Remember the first time you gazed through the night sky filled with twinkling stars? Perhaps, you were a little child lying on your Dadi’s lap, listening to lovely stories as you gazed your way through the abyss. Or perhaps it was the time when you gazed outside the car window as your Papa drove swiftly on the road.\n Photo by Vincentiu Solomon on Unsplash\nRemember the first time you rode your bicycle? The feelings that you felt- feelings of excitement, alertness, nervousness, and especially the feeling when you felt you got it as you peddled your way through the road shouting Yoohooooo!!\n Photo by Flo Karr on Unsplash\nThere is something magical about our first times. First time doing or experiencing something.\nThis is my first time blogging! Amongst the plethora of feelings that I am feeling right now, are nervousness and excitement. Nervous because I do not know what to write and I do not know how this would all turn up in the end. Excited because I think this can be the start of something great!\nI opened this draft, had a sip of tea, placed my hands on the keyboard, and thought about what to write. Then all of this sort of came naturally to me. Lucky me!\nI guess Khaled Hosseini was right when he said that he would sit at his writing desk and wish for something magical to happen!\nThe first time I realized that I had a knack for writing(or maybe I don’t) was when I was in high school, in 10th grade. There was this English test that had a letter-writing section, and if I remember correctly the theme was something around “Write a letter to the MLA about helping orphans”. One thing I clearly remember about writing that letter, although it was for a test, was that I captured the pain and emotions of orphans and poured it out. For me, it felt real. I even thought of writing an actual letter later and sending it to my local MLA to help out the orphans in my locality. Anyways, after we handed over our test papers, I got feedback from my English teacher who was amazed by what I wrote. She was impressed by my writing style and the depth of my clarity. That sort of stayed with me. I never got a chance to actually write something meaningful for the next some years until I entered college. During those years, I used to volunteer at an NGO on Sundays as an education volunteer teaching kids from slum areas Math & English. During one of those times, I had written a post that captured the feelings of those kids when we the volunteers would come and meet them every Sunday. I wrote it and shared it with my co-volunteers. They all loved it! That is when I got a sense of self-awareness that there is a writer that is hidden inside me.\nI am starting this blog platform to allow myself to be that writer. I do not wish to get fame or anything, but I just want to write my heart out!\nOne of the things that scare me to hell is when I die and I am in my grave, my tombstone shouldn’t say “A writer buried with his words”.\nSo this is the beginning of my journey to do something that I love, to do something that makes me happy, to do something that makes me feel at home, to do something for the sake of its beauty, to do something for the sake of my passion- to write! And I invite you as well my friend, to start doing what your heart has been saying to you for all these years.\n\n“It is never too late to be what you might have been” - George Elliot\n\nSee ya around!"
  },
  {
    "objectID": "posts/segmentation/segmentation.html",
    "href": "posts/segmentation/segmentation.html",
    "title": "Oh! Segmentation Tasks, Segmentation Tasks",
    "section": "",
    "text": "Street view image created using Stable Diffusion\n\n\nCan you recognize various objects from the above picture? Our eyes have the ability to see a complete image and recognize the distinguishing objects from it. We know there is a car on the left, a person ahead on the right, etc. Can we make computers do this?! Let us find out!"
  },
  {
    "objectID": "posts/segmentation/segmentation.html#introduction",
    "href": "posts/segmentation/segmentation.html#introduction",
    "title": "Oh! Segmentation Tasks, Segmentation Tasks",
    "section": "",
    "text": "Street view image created using Stable Diffusion\n\n\nCan you recognize various objects from the above picture? Our eyes have the ability to see a complete image and recognize the distinguishing objects from it. We know there is a car on the left, a person ahead on the right, etc. Can we make computers do this?! Let us find out!"
  },
  {
    "objectID": "posts/segmentation/segmentation.html#what-is-segmentation",
    "href": "posts/segmentation/segmentation.html#what-is-segmentation",
    "title": "Oh! Segmentation Tasks, Segmentation Tasks",
    "section": "What Is Segmentation?",
    "text": "What Is Segmentation?\nSegmentation refers to the process of dividing an image into multiple regions or segments, where each segment corresponds to a specific object or background.\nThe goal of a segmentation model in Deep Learning is to recognize the content of every individual pixel in an image thereby localizing objects in it.\nThis task is very important for self-driving cars, for example. If a self-driving car doesn’t know where a pedestrian is, then it doesn’t know how to avoid one!"
  },
  {
    "objectID": "posts/segmentation/segmentation.html#building-a-segmentation-model",
    "href": "posts/segmentation/segmentation.html#building-a-segmentation-model",
    "title": "Oh! Segmentation Tasks, Segmentation Tasks",
    "section": "Building A Segmentation Model",
    "text": "Building A Segmentation Model\nWe will train a simple segmentation model using the fastai library.\n\n\n\nSource: https://giphy.com/gifs/starwars-star-wars-the-last-jedi-xT9Iguc1FSPtLmCw5W\n\n\n\nInstalling & Importing Relevant Libraries\nWe will install the fastai software and export all the vision libraries, since this is computer vision task.\n\n!pip install -Uqqq fastai\n\n\nfrom fastai.vision.all import *\n\n\n\nGetting Training Data\nWe will use a subset of the CamVid dataset from the paper “Semantic Object Classes in Video: A High-Definition Ground Truth Database” by Gabriel J. Brostow et al.\nThe following code gets the data for us!\n\npath = untar_data(URLs.CAMVID_TINY)\npath\n\n\n\n\n\n\n    \n      \n      100.18% [2318336/2314212 00:00&lt;00:00]\n    \n    \n\n\nPath('/root/.fastai/data/camvid_tiny')\n\n\nNow that we got the data, before using it for training our model, we we will have to specify fastai what kind of data we have, how it is structured, and for what task are we going to use it.\nThe following code does this.\n\ndls = SegmentationDataLoaders.from_label_func(path, bs = 8, fnames = get_image_files(path/'images'),\n                                             label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n                                             codes = np.loadtxt(path/'codes.txt', dtype = str))\n\nLet us have a look at our data!\n\ndls.show_batch()\n\n\n\n\nThese are the 8 images for which we will train a model that can recognize its objects.\n\n\nUsing A Learner & Fine-tuning\nWe will use a unet learner and resenet34 architecture, a pre-trained model to fine-tune it for our task of segmentation.\n\nlearner = unet_learner(dls, resnet34)\nlearner.fine_tune(10)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 298MB/s]\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n2.925367\n1.908659\n00:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.629299\n1.505528\n00:01\n\n\n1\n1.446579\n1.181775\n00:01\n\n\n2\n1.380383\n1.429530\n00:01\n\n\n3\n1.333090\n1.063483\n00:01\n\n\n4\n1.237765\n1.000342\n00:01\n\n\n5\n1.134848\n0.884875\n00:01\n\n\n6\n1.037858\n0.903598\n00:01\n\n\n7\n0.948801\n0.812336\n00:01\n\n\n8\n0.874415\n0.829520\n00:01\n\n\n9\n0.816568\n0.826488\n00:01\n\n\n\n\n\nThe training is done! It trained for about 15-20 sec. \nLet us see the results!\n\nlearner.show_results(figsize = (7, 8))\n\n\n\n\n\n\n\n\n\n\n\nVoila! On the left are the original images, and on the right are the ones that were segmented by our model. We can clearly see that it did a pretty good job in recognizing the pixels! For some, it got a few things wrong, but for most of them it got right!\nTo summarize, computers now can identify objects in an image, that too pretty well!"
  },
  {
    "objectID": "posts/segmentation/segmentation.html#a-poem",
    "href": "posts/segmentation/segmentation.html#a-poem",
    "title": "Oh! Segmentation Tasks, Segmentation Tasks",
    "section": "A Poem",
    "text": "A Poem\nHere is a poem generated by ChatGPT on Segmentation.\n\nOh, segmentation task, segmentation task, Dividing images is its awesome task. To split an image into regions fine, And label each one, that is the line.\nSegmentation finds objects in a snap, And separates them from the background’s trap. In autonomous cars it plays a role, To avoid obstacles and reach the goal.\nIn medical images it helps a lot, To pinpoint where the issues are wrought. Convolutional neural networks take charge, And extract features like a boss, it’s large!\nThey label each pixel with a keen eye, And categorize them, oh so sly. Object recognition, it can aid, And much more, the list won’t fade.\nSegmentation task, oh how grand, In computer vision, it’s in high demand! Precise and accurate, it must be, So many applications, it sets us free!"
  },
  {
    "objectID": "posts/continents_classifier/continents_classifier.html",
    "href": "posts/continents_classifier/continents_classifier.html",
    "title": "Teaching AI Some Geography",
    "section": "",
    "text": "Lesson 1 of the Practical Deep Learning for Coders course offered by fast.ai gets hands-on on building an image classifier of your choice pretty quickly without any hastles from the get-go!\nI tried to build an image classifier that could classify the 7 continents based on their map images. And boy let me tell you, I was unbelievably amazed by its results.\nLet me show you how I did it! Feel free to try yourself on Colab or any platform of your choice!"
  },
  {
    "objectID": "posts/continents_classifier/continents_classifier.html#setting-up",
    "href": "posts/continents_classifier/continents_classifier.html#setting-up",
    "title": "Teaching AI Some Geography",
    "section": "Setting Up",
    "text": "Setting Up\n\nInstall Packages\nWe begin by installing fastbook and duckduckgo_search. The formers allows us to access the fast.ai library that will help us build our image classifier and the latter allows us to access DuckDuckGo search engine features to search, get, and download our images for training.\n\n!pip install -Uqq fastbook duckduckgo_search\n\n\n\nImport Relevant Libraries\nWe import the relevant libraries. More about them here.\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\n\n\nSetup A System For Getting Images\nHere, we create a handy little function that can search for a specific term on DuckDuckGo image searches and get its URLs\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nFor example, if our term is africa continent map, the above function gets the first popped up URL of the image of africa continent map as shown below.\n\nurls = search_images('africa continent map', max_images=1)\nurls[0]\n\nSearching for 'africa continent map'\n\n\n'https://cdn.onestopmap.com/wp-content/uploads/2015/05/464-map-africa-continent-political-shaded-relief.jpg'\n\n\nOnce, we get the URLs of an image, let us setup a way through which we can actually download the URLs we obtained and view them. We will storing the downloaded files in jpg formats.\nThe following snippets do that for us.\n\ndest = 'africa.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\nThe Africa continent looks good. Let us have sneak peak on the remaining 6 continents just to make sure that DuckDuckGo is getting the right images for us.\n\ndownload_url(search_images('antarctica continent map', max_images=1)[0], 'antarctica.jpg', show_progress=False)\nImage.open('antarctica.jpg').to_thumb(256,256)\n\nSearching for 'antarctica continent map'\n\n\n\n\n\n\ndownload_url(search_images('asia continent map', max_images=1)[0], 'asia.jpg', show_progress=False)\nImage.open('asia.jpg').to_thumb(256,256)\n\nSearching for 'asia continent map'\n\n\n\n\n\n\ndownload_url(search_images('australia continent map', max_images=1)[0], 'australia.jpg', show_progress=False)\nImage.open('australia.jpg').to_thumb(256,256)\n\nSearching for 'australia continent map'\n\n\n\n\n\n\ndownload_url(search_images('europe continent map', max_images=1)[0], 'europe.jpg', show_progress=False)\nImage.open('europe.jpg').to_thumb(256,256)\n\nSearching for 'europe continent map'\n\n\n\n\n\n\ndownload_url(search_images('north america continent map', max_images=1)[0], 'north_america.jpg', show_progress=False)\nImage.open('north_america.jpg').to_thumb(256,256)\n\nSearching for 'north america continent map'\n\n\n\n\n\n\ndownload_url(search_images('south america continent map', max_images=1)[0], 'south_america.jpg', show_progress=False)\nImage.open('south_america.jpg').to_thumb(256,256)\n\nSearching for 'south america continent map'\n\n\n\n\n\nEverything, looks good!\nNow that everything is setup, let us create our classifier. Generally, the following steps are taken when building an image classifier model."
  },
  {
    "objectID": "posts/continents_classifier/continents_classifier.html#get-image-data",
    "href": "posts/continents_classifier/continents_classifier.html#get-image-data",
    "title": "Teaching AI Some Geography",
    "section": "Get Image Data",
    "text": "Get Image Data\nLet us go on and get our image data.\n\nsearches = ['africa', 'antarctica', 'asia', 'australia', 'europe', 'north america', 'south america']\npath = Path('which_continent_is_it')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} continent map'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'africa continent map'\nSearching for 'antarctica continent map'\nSearching for 'asia continent map'\nSearching for 'australia continent map'\nSearching for 'europe continent map'\nSearching for 'north america continent map'\nSearching for 'south america continent map'\n\n\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:1015: UserWarning: Couldn't allocate palette entry for transparency\n  warnings.warn(\"Couldn't allocate palette entry for transparency\")\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/usr/local/lib/python3.7/dist-packages/PIL/Image.py:1015: UserWarning: Couldn't allocate palette entry for transparency\n  warnings.warn(\"Couldn't allocate palette entry for transparency\")\n\n\nSometimes, we might get a few broken images. We can track and discard them as follows.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n7"
  },
  {
    "objectID": "posts/continents_classifier/continents_classifier.html#create-datablock",
    "href": "posts/continents_classifier/continents_classifier.html#create-datablock",
    "title": "Teaching AI Some Geography",
    "section": "Create DataBlock",
    "text": "Create DataBlock\nNext we create a DataBlock that helps us put our images into the model.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=21)"
  },
  {
    "objectID": "posts/continents_classifier/continents_classifier.html#create-the-learner",
    "href": "posts/continents_classifier/continents_classifier.html#create-the-learner",
    "title": "Teaching AI Some Geography",
    "section": "Create The Learner",
    "text": "Create The Learner\nNow, we create the actual model that will learn from the image data we gave it.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.544921\n1.500386\n0.538462\n00:44\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.187040\n0.556937\n0.179487\n01:04\n\n\n1\n0.718358\n0.290570\n0.089744\n01:04\n\n\n2\n0.497790\n0.240653\n0.076923\n01:02"
  },
  {
    "objectID": "posts/continents_classifier/continents_classifier.html#test-the-model",
    "href": "posts/continents_classifier/continents_classifier.html#test-the-model",
    "title": "Teaching AI Some Geography",
    "section": "Test The Model",
    "text": "Test The Model\nOur model is ready to be tested!\nLet us input the images we stored at the beginning and see its results.\n\nwhich_continent_is_it,_,probs = learn.predict(PILImage.create('africa.jpg'))\nprint(f\"This is: {which_continent_is_it}.\")\nprint(f\"Probability: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is: africa.\nProbability: 0.9926\n\n\n\nwhich_continent_is_it,_,probs = learn.predict(PILImage.create('antarctica.jpg'))\nprint(f\"This is: {which_continent_is_it}.\")\nprint(f\"Probability: {probs[1]:.4f}\")\n\n\n\n\n\n\n\n\nThis is: antarctica.\nProbability: 0.9998\n\n\n\nwhich_continent_is_it,_,probs = learn.predict(PILImage.create('asia.jpg'))\nprint(f\"This is: {which_continent_is_it}.\")\nprint(f\"Probability: {probs[2]:.4f}\")\n\n\n\n\n\n\n\n\nThis is: asia.\nProbability: 1.0000\n\n\n\nwhich_continent_is_it,_,probs = learn.predict(PILImage.create('australia.jpg'))\nprint(f\"This is: {which_continent_is_it}.\")\nprint(f\"Probability: {probs[3]:.4f}\")\n\n\n\n\n\n\n\n\nThis is: australia.\nProbability: 0.9995\n\n\n\nwhich_continent_is_it,_,probs = learn.predict(PILImage.create('europe.jpg'))\nprint(f\"This is: {which_continent_is_it}.\")\nprint(f\"Probability: {probs[4]:.4f}\")\n\n\n\n\n\n\n\n\nThis is: europe.\nProbability: 0.9992\n\n\n\nwhich_continent_is_it,_,probs = learn.predict(PILImage.create('north_america.jpg'))\nprint(f\"This is: {which_continent_is_it}.\")\nprint(f\"Probability: {probs[5]:.4f}\")\n\n\n\n\n\n\n\n\nThis is: north america.\nProbability: 0.9999\n\n\n\nwhich_continent_is_it,_,probs = learn.predict(PILImage.create('south_america.jpg'))\nprint(f\"This is: {which_continent_is_it}.\")\nprint(f\"Probability: {probs[6]:.4f}\")\n\n\n\n\n\n\n\n\nThis is: south america.\nProbability: 0.9978\n\n\nLooks pretty impressive isn’t it?!\nIf you have any ideas of your own feel free to try them out."
  },
  {
    "objectID": "posts/linux_subsystem/index.html",
    "href": "posts/linux_subsystem/index.html",
    "title": "Running A Linux Subsystem In Windows",
    "section": "",
    "text": "One of the many first steps when venturing into doing Data Science is to have access to a Unix/Linux/Ubuntu OS.\nBut having a Windows OS and wanting to setup and run this hardcore geeky OS might be challenging.\nIn this blog, I will show a very quick and a simple way of setting up a Linux Subsystem in your Windows OS that I learned from Jeremy Howard! If you wish to follow along, you will have a complete full fledge Linux OS running in your Windows in the next 3-5 minutes. No complicated hacking stuff, no dual booting, no splitting of your computer into two halves using a light beam, none of that stuff. And if you are wondering if you will lose all the Windows features, don’t worry, you won’t.\nLet us get started!\n\n\n\nSource: https://giphy.com/gifs/wikitude-augmented-reality-penguin-linux-4Zgy9QqzWU8C3ugvCa"
  },
  {
    "objectID": "posts/linux_subsystem/index.html#introduction",
    "href": "posts/linux_subsystem/index.html#introduction",
    "title": "Running A Linux Subsystem In Windows",
    "section": "",
    "text": "One of the many first steps when venturing into doing Data Science is to have access to a Unix/Linux/Ubuntu OS.\nBut having a Windows OS and wanting to setup and run this hardcore geeky OS might be challenging.\nIn this blog, I will show a very quick and a simple way of setting up a Linux Subsystem in your Windows OS that I learned from Jeremy Howard! If you wish to follow along, you will have a complete full fledge Linux OS running in your Windows in the next 3-5 minutes. No complicated hacking stuff, no dual booting, no splitting of your computer into two halves using a light beam, none of that stuff. And if you are wondering if you will lose all the Windows features, don’t worry, you won’t.\nLet us get started!\n\n\n\nSource: https://giphy.com/gifs/wikitude-augmented-reality-penguin-linux-4Zgy9QqzWU8C3ugvCa"
  },
  {
    "objectID": "posts/linux_subsystem/index.html#step-0-getting-a-terminal",
    "href": "posts/linux_subsystem/index.html#step-0-getting-a-terminal",
    "title": "Running A Linux Subsystem In Windows",
    "section": "Step 0: Getting A Terminal",
    "text": "Step 0: Getting A Terminal\nWe will need a terminal handy right after we are done setting up Linux! There are many options to choose from. Let us download and install the Windows Terminal from the Microsoft Store before proceeding.\n\nPress the “⊞” key and type “store”.\n\n\n\nMicrosoft Store search\n\n\nOpen the Microsoft Store and search for “windows terminal”.\n\n\n\nDownload & install Windows Terminal\n\n\nDownload, and install it. We will use this soon!"
  },
  {
    "objectID": "posts/linux_subsystem/index.html#step-1-getting-the-linux-subsystem",
    "href": "posts/linux_subsystem/index.html#step-1-getting-the-linux-subsystem",
    "title": "Running A Linux Subsystem In Windows",
    "section": "Step 1: Getting the Linux Subsystem",
    "text": "Step 1: Getting the Linux Subsystem\nWe will be setting up our subsystem from the Windows Powershell application.\n\nPress the “⊞” key button and search for “powershell”.\n\n\n\nPowershell search\n\n\nClick on Run as Administrator. It will prompt to verify. Click Yes.\nOnce you are inside the Windows Powershell, type the wsl --install command and press “⏎”.\n\n\n\nwsl-command\n\n\nThis will download and install 2 things, the Windows Subsystem for Linux and Ubuntu as shown below. To take effect we need to reboot our system.\nAfter the installation is done, close all the files and applications, and restart the computer."
  },
  {
    "objectID": "posts/linux_subsystem/index.html#step-2-hello-linux",
    "href": "posts/linux_subsystem/index.html#step-2-hello-linux",
    "title": "Running A Linux Subsystem In Windows",
    "section": "Step 2: Hello, Linux",
    "text": "Step 2: Hello, Linux\nUpon restarting your computer a powershell window will pop up automatically, welcoming you and launching the Linux Subsystem. Once it is launched, it will ask to set up a username and a password. This is like a new computer that is running!\n\nPut a username and a password and you are done!\n\n\n\nAll set\n\n\n\nWhat you have now is a full fledged Linux Subsystem running in your Windows."
  },
  {
    "objectID": "posts/linux_subsystem/index.html#step-3-accessing-linux",
    "href": "posts/linux_subsystem/index.html#step-3-accessing-linux",
    "title": "Running A Linux Subsystem In Windows",
    "section": "Step 3: Accessing Linux",
    "text": "Step 3: Accessing Linux\nRemember that Windows Terminal we kept ready at the beginning?! We will be using that to acsess our Linux CLI.\n\nOpen the Windows Terminal. By default, the Windows Powershell will run.\n\n\n\nPowershell as default\n\n\nThere will be a little arrown pointing as shown. Click it, and will show the list of options we can select from. To get Linux, we will have to select manually from this list. Instead, let us setup Linux as the default profile whenever we open our Windows Terminal. To do that and go to Settings.\n\n\n\nSettings\n\n\nMake Linux or Ubuntu as the default one.\n\n\n\nmaking Linux/Ubuntu as default\n\n\n\nNext time you open the Windows Terminal, it will welcome you with Linux/Ubuntu.\n\n\n\n\n\n\nLinux\n\n\n\n\n\n\n\nUbuntu"
  },
  {
    "objectID": "posts/linux_subsystem/index.html#explore",
    "href": "posts/linux_subsystem/index.html#explore",
    "title": "Running A Linux Subsystem In Windows",
    "section": "Explore",
    "text": "Explore\nYou are all set to dwell into the Linux metaverse! If you are new to Linux, check out some tutorials, like this one to learn about it."
  },
  {
    "objectID": "posts/on_blogging/index.html",
    "href": "posts/on_blogging/index.html",
    "title": "Overcome The Barriers Of Blogging",
    "section": "",
    "text": "It has been around 6 months since my last blog post.\nI was very excited when I started this blog. I was inspired by two posts, this and this. I thought of a cool name theaeonwanderer, checked out the Quarto guide, and went on to set up my blog. It felt really amazing. I had this personal web space of mine, that I only imagined! I had a platform for myself, a place where I can share stuff.\nI wrote two posts around the fast.ai materials I was learning back then when I started. Shared on the forums, got feedbacks, it was great! In the meantime a lot had happened with my learning journey but one of the things that created a friction for me when it came to blogging was my psychological and emotional barriers. I was thinking(and feeling) too much. There were folks, who were writing highly technical stuff talking about papers and so on! And there I was trying to understand what fine-tuning is, struggling to deploy my model in Lesson 2, and barely grasping the concepts taught in the lessons. I said to myself, that I will focus just on learning and one day, when I am over it, I will start blogging about these higher level stuff. Boy was I so wrong!\nI was thinking who the hell would benefit from this. It seemed all uncool and hence I wasn’t embracing it(quite the opposite of the fast.ai slogan eh?!).\nI remember Jeremy saying the following in Lesson 0:\nAs an irony, this exact blog post is that! I wish I knew back then what I am about to cover in this blog. It took me 6 months to get over my psychological and emotional barriers! A few traces still exists but I will overcome them soon."
  },
  {
    "objectID": "posts/on_blogging/index.html#rome-wasnt-built-in-a-day",
    "href": "posts/on_blogging/index.html#rome-wasnt-built-in-a-day",
    "title": "Overcome The Barriers Of Blogging",
    "section": "Rome Wasn’t Built In A Day",
    "text": "Rome Wasn’t Built In A Day\nOne of the important pieces of advice I was given by my Professor when I was beginning to learn coding was to complete 200 hours of coding and then think about the intermediate or advanced stuff. It is only after you have done the 200 hours of coding, you will get out of that beginner layman/noob phase. Now, would it be wise to judge yourself while you are at the 2nd hour of coding? No not at all!\nThe same goes with any learning venture. The same goes for blogging. There is always an initial phase where we are all noobs. We are horrible at it! You can not stop then. Instead you must allow yourself to just go through the process. Embrace the falls, embrace the weaknesses, embrace the shitty blog posts! Let a few months pass, let a year pass and you will be better at it!\nAt the start of your blogging journey you must allow yourself to write messy posts. It is only after you have written a certain amount of posts, you will learn to improve in your style of writing as well as the content!\nYouTubers when starting their journeys are often told that their first 50-100 videos will suck! They stick with it! Go and look at the earlier videos of PewDiePie or any other successful YouTuber. Even your favourite bloggers, started from way down. What we see today, is a result of the work they put in consistently. That is how learning happens. We are humans after all!\nRemember the journey of Deep Learning?! Back in the 1950s, we started with what, a simple model of an artificial neuron?! And today we have Chat-GPT4 and Stable Diffusion."
  },
  {
    "objectID": "posts/on_blogging/index.html#what-to-blog-about",
    "href": "posts/on_blogging/index.html#what-to-blog-about",
    "title": "Overcome The Barriers Of Blogging",
    "section": "What To Blog About?",
    "text": "What To Blog About?\nThere is one simple idea, that I am going to apply myself and would encourage you to apply as well.\nRight now, you are learning Deep Learning right?! How are you doing that? Probably through the Practical Deep Learning for Coders course. What does it have? It has the following:\n\nLecture videos\nTextbook chapters\nNBs\nResources\n\nYou immerse yourself in the learning process. You watch the lecture, read the chapter, experiment with code, all that wonderful stuff! Once you have learned something, from the lecture, or the chapter, or through experimenting, and even if that learning is as small as concept or as big as building an NLP model, You teach it! Consider it your job! You have to teach it. It has numerous benefits for you as well for others! For you, among other benefits, it helps you solidify what you have learned and clears the gaps in your understanding. For others who are a few steps behind you on the learning journey, it might help them.\nThe idea is the following:\n\nUse blogging as a medium to enhance your learning, helping yourself and others by teaching or explaining it!\n\nDid you learn about validation set and test set? Teach it! Did you build an image classifier? Show us how you did it! Did you understand the code behind that image classifier you built? Link the previous blog and explain the code! Did you feel stuck in the middle of your lesson and got yourself out of it? Share that experience.\nWhenever you learn anything new, just explain it!\nFor other benefits, refer to the links I share at the end of this blog."
  },
  {
    "objectID": "posts/on_blogging/index.html#blogs-that-inspire-me",
    "href": "posts/on_blogging/index.html#blogs-that-inspire-me",
    "title": "Overcome The Barriers Of Blogging",
    "section": "Blogs That Inspire Me",
    "text": "Blogs That Inspire Me\nFinally, I am going to leave you with some of the blogs that have inspired and encouraged me and I hope that they inspire and encourage you as well.\n\nForBo7 // Salman Naqvi - Welcome to the world of ForBo7\nAman Arora\nKurian Benoy\nWasim Lorgat\nAbout Tanishq - Tanishq Mathew Abraham\nHamel’s Blog\nFinally, our beloved fast.ai - fast.ai—Making neural nets uncool again"
  },
  {
    "objectID": "posts/on_blogging/index.html#resources",
    "href": "posts/on_blogging/index.html#resources",
    "title": "Overcome The Barriers Of Blogging",
    "section": "Resources",
    "text": "Resources\n\nQuarto\nLesson “0”: Practical Deep Learning for Coders (fast.ai) - YouTube\nMeta Learning: How To Learn Deep Learning And Thrive In The Digital World\nIntroduction to Quarto | Blogging, Knowledge Sharing, Writing a Book - YouTube\nHow to Blog to Advance Your Career and Learn Faster - YouTube\nfast.ai - Advice for Better Blog Posts\nWhy you (yes, you) should blog. The top advice I would give my younger… | by Rachel Thomas | Medium\nfast.ai - Making Peace with Personal Branding"
  },
  {
    "objectID": "posts/frolicking_with_ai_generated_art/index.html",
    "href": "posts/frolicking_with_ai_generated_art/index.html",
    "title": "Frolicking With AI Generated Art",
    "section": "",
    "text": "We are living in the times where all art forms that were solely reserved for humans is slowly being generated by AI. Be it the art of writing, music, images, video creation, and even pure art!\n2022 began wonderfully with the advent of AI systems that could create art with mere textual descriptions. Platforms like DALL.E.2, Mindjourney, and many more have emerged gaining high popularity across the world.\nI was a little away from the AI buzz news, and I got to know about it a little later when I was listening to Lesson 1 of the Practical Deep Learning for Coders course offered by fast.ai. Jeremy Howard, the instructor of the course shared these amazing explorations done by people who toyed with this new state of the art AI beauty!\nI had to go and explore them myself. I just had to! I mean, just look at these.\nSo, I signed up of DALL.E.2 and started to experiment with it. In this blog I am sharing some of my explorations which were mind bending."
  },
  {
    "objectID": "posts/frolicking_with_ai_generated_art/index.html#a-challenge-i-faced",
    "href": "posts/frolicking_with_ai_generated_art/index.html#a-challenge-i-faced",
    "title": "Frolicking With AI Generated Art",
    "section": "A Challenge I Faced",
    "text": "A Challenge I Faced\nI had this wonderful machine infront of me, that was waiting for my textual instructions to create art! The first challenge I faced was the inability to express what I wanted to see. I mean, I am a human after all, and I must be really good with the primary skills that I possess as a human, to articulate myself. But, there I was staring blankly at the search bar, thinking what to type. All I could come up with was “fox eating an ice cream”, which is a good thing to begin with. It did give me results of “a fox eating an ice cream”.\n\n\n\nDALL.E.2 Illustration\n\n\nBut I couldn’t articulate longer descriptive sentences. I spent some time thinking and grabbing ideas from other peoples search styles and finally I was able to describe something expressive and detailed. Let me show you its wonders."
  },
  {
    "objectID": "posts/frolicking_with_ai_generated_art/index.html#my-dall.e.2-explorations",
    "href": "posts/frolicking_with_ai_generated_art/index.html#my-dall.e.2-explorations",
    "title": "Frolicking With AI Generated Art",
    "section": "My DALL.E.2 Explorations",
    "text": "My DALL.E.2 Explorations\n\nabstract portrait of a man made with flowers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nabstract art of a robot teaching math to kids\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscholastic deep explorations in snow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3d render of a book made with melting cheese placed on dining table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na flower riding a horse in space meeting aliens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na dog wearing glasses teaching robots in a class\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsamurai fighting a battle with a katana on fire\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIsn’t it amazing?! Feel free to try yourself too!"
  },
  {
    "objectID": "posts/frolicking_with_ai_generated_art/index.html#a-minor-epiphany",
    "href": "posts/frolicking_with_ai_generated_art/index.html#a-minor-epiphany",
    "title": "Frolicking With AI Generated Art",
    "section": "A Minor Epiphany",
    "text": "A Minor Epiphany\nI might be stepping on a slightly contreversial theme here, but on a bit of a different philosophical tangent, I had a minor epiphany that I want to share.\nAs I was struggling to articulate descriptions of art I wanted see, it crossed my mind that although we are moving towards this amazing world of possibilities where AI can do brilliantly magical stuff, but we as humans are also losing parts of ourselves which were innate to our cognitive abilities. We are in a sense delegating our cognitive tools to AI and dumbing ourselves down. I mean what will be left of us when every possible capability of our minds is transfered to AI. There were times, when we would smirk at the concept of doing mental arithmetics thinking that our mobile phones can do it for us, which they can for sure, very accurately. But we are losing our mental abilities to machines to a point that we can not even use it the way we want.\nTo end on a lighter note, here are a few art to cheer you up!\n\n\nfox walking down the street to a bar on a rainy night holding an umbrella"
  },
  {
    "objectID": "posts/colored_cups/index.html#introduction",
    "href": "posts/colored_cups/index.html#introduction",
    "title": "Colored Cups",
    "section": "Introduction",
    "text": "Introduction\nI have had my share in teaching high school students a few years ago and one of the challenges that I faced back then was of getting a real time feedback of my student’s learning, as I was teaching in the class. Since I could not (and can not now as well) read their minds to know if they are following well along, I had to rely on them. They had to let me know if they are following along well. That brings another challenge! A student first must be self aware of his/her learning. In order to enable this, the students must be made owners of their learning."
  },
  {
    "objectID": "posts/colored_cups/index.html#colored-cups",
    "href": "posts/colored_cups/index.html#colored-cups",
    "title": "Colored Cups",
    "section": "Colored Cups",
    "text": "Colored Cups\nColored Cups introduced by Dylan Wiliam in his book Embedded Formative Assessment (2011), is an effective technique that can be used in a classroom that attempts to:\n\nGet real-time feedback of student’s learning for teachers.\nPromotes self-regulated learning for students."
  },
  {
    "objectID": "posts/colored_cups/index.html#working",
    "href": "posts/colored_cups/index.html#working",
    "title": "Colored Cups",
    "section": "Working",
    "text": "Working\nThe idea is simple!\nAll the students in the classrooms have 3 colored cups, a green cup, a yellow cup, and a red cup which are stacked together at the beginning of the class.\nEach of these colors reflect the various levels of understanding. Green means the student is understanding well, yellow means s/he is not quite sure what is going on, and red means the student has no idea of what is going on.\nAs the lesson is being taught, the students put a colored cup on their desks that corresponds to how they are following along, and the teacher can see the classroom to get a sense of how the students are following along.\nPrior to when a student puts a colored cup on the desk, s/he must self-assess his learning."
  },
  {
    "objectID": "posts/colored_cups/index.html#adaptations-in-the-fast.ai-course",
    "href": "posts/colored_cups/index.html#adaptations-in-the-fast.ai-course",
    "title": "Colored Cups",
    "section": "Adaptations In The fast.ai Course",
    "text": "Adaptations In The fast.ai Course\nI first got to know about this in Practical Deep Learning for Coders course. In the course, a virtual setup was made and the teacher, Jeremy could see it from his end in the teacher version."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am self learning Data Science, which encapsulates multiple fields. I am currently exploring Deep Learning, Machine learning, and AI. I wish to work on building cool AI systems someday.\nOn this blog I document my learning journey and explorations in Data Science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Overcome The Barriers Of Blogging\n\n\n\n\n\n\n\nexperiences\n\n\ntips\n\n\n\n\nSharing some honest experiences and tips on blogging\n\n\n\n\n\n\nMay 27, 2023\n\n\nAbdullah Zeeshan\n\n\n\n\n\n\n  \n\n\n\n\nOh! Segmentation Tasks, Segmentation Tasks\n\n\n\n\n\n\n\ncomputer vision\n\n\n\n\nCan computers detect objects in an image?\n\n\n\n\n\n\nDec 5, 2022\n\n\nAbdullah Zeeshan\n\n\n\n\n\n\n  \n\n\n\n\nColored Cups\n\n\n\n\n\n\n\neducation\n\n\nfast.ai\n\n\n\n\nAn effective tool for every classroom\n\n\n\n\n\n\nNov 27, 2022\n\n\nAbdullah Zeeshan\n\n\n\n\n\n\n  \n\n\n\n\nRunning A Linux Subsystem In Windows\n\n\n\n\n\n\n\ntutorial\n\n\nlinux\n\n\n\n\nA quick guide\n\n\n\n\n\n\nNov 24, 2022\n\n\nAbdullah Zeeshan\n\n\n\n\n\n\n  \n\n\n\n\nTeaching AI Some Geography\n\n\n\n\n\n\n\nfast.ai\n\n\nexperiments\n\n\n\n\nBuilding an image classifier\n\n\n\n\n\n\nNov 14, 2022\n\n\nAbdullah Zeeshan\n\n\n\n\n\n\n  \n\n\n\n\nFrolicking With AI Generated Art\n\n\n\n\n\n\n\nnews\n\n\ngeneral\n\n\nexplorations\n\n\n\n\nExploring DALL.E.2\n\n\n\n\n\n\nNov 12, 2022\n\n\nAbdullah Zeeshan\n\n\n\n\n\n\n  \n\n\n\n\nOur First Times\n\n\n\n\n\n\n\nwriting\n\n\n\n\nMy first blog\n\n\n\n\n\n\nNov 11, 2022\n\n\nAbdullah Zeeshan\n\n\n\n\n\n\nNo matching items"
  }
]